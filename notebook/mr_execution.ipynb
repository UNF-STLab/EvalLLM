{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install sentence_transformers\n",
    "%pip install pubchempy\n",
    "%pip install chemdataextractor\n",
    "import nltk\n",
    "%nltk.download('wordnet')\n",
    "%nltk.download('omw-1.4')\n",
    "%nltk.download('stopwords')\n",
    "%nltk.download('punkt')\n",
    "%cde data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from random import randint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from chemdataextractor import Document\n",
    "import pubchempy as pubc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(values : list, filepath : str) -> None:\n",
    "    '''Function to write output on the csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values : list\n",
    "      List of values to be written on the csv\n",
    "    filepath : str\n",
    "      Path of the csv file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> write_to_csv(['value1', 'value2', 'value3'], 'output.csv')\n",
    "    This will append the row ['value1', 'value2', 'value3'] to 'output.csv'.\n",
    "    '''\n",
    "    if not isinstance(values, list) or not values:\n",
    "        raise ValueError(\"The 'values' parameter must be a non-empty list.\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'a', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(values)\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"An error occurred while writing to the file {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text : str) -> str:\n",
    "   '''\n",
    "    Clean up the input data by applying preprocessing steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input string that will be processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The preprocessed string after applying the cleanup operations.\n",
    "    '''\n",
    "   tokens = word_tokenize(text)\n",
    "   tokens = [w for w in tokens if w not in punctuation and not w.isdigit() and not len(w) < 3]\n",
    "   stop_words = stopwords.words ('english')\n",
    "   tweet_without_stopwords = [t for t in tokens if t not in stop_words]\n",
    "   text = \" \".join (tweet_without_stopwords)\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_score(s1,s2, vec):\n",
    "    '''\n",
    "    Calculate the cosine similarity score between two input strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input string on which preprocessing should be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The preprocessed string. \n",
    "    '''\n",
    "    s1 = preprocessing(s1)\n",
    "    s2 = preprocessing(s2)\n",
    "    sentences = [s1,s2]\n",
    "    sentence_to_vec = vec.fit_transform(sentences)\n",
    "    sentence_to_vec_arr = sentence_to_vec.toarray()\n",
    "    sim_score = cosine_similarity(sentence_to_vec_arr)\n",
    "    return round(sim_score[0][1],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_similarity_score(s1,s2, model):\n",
    "    '''\n",
    "    Computes the semantic similarity score between two input words/strings using a transformer model.\n",
    "\n",
    "    Args:\n",
    "        s1 (str): The first input word/string.\n",
    "        s2 (str): The second input word/string.\n",
    "        model (obj): The transformer model used to encode the strings into embeddings. \n",
    "            This model should have an `encode` method that returns tensor embeddings.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity score between the embeddings of the two input word/strings,\n",
    "            rounded to three decimal places. The score ranges from 0.0 (completely dissimilar)\n",
    "            to 1.0 (completely similar).\n",
    "    '''\n",
    "    embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "    semantic_similarity_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    return round(float(semantic_similarity_scores[0][0]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(input_file_path, word_count, similarity_tsd):\n",
    "    pmr = 'swap_word'\n",
    "    mr = f'{pmr}_{word_count}_{similarity_tsd}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'swapped_word_pos', 'word_swapped', 'swapped_word'],log_file_path)\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    sentence_count = 0\n",
    "    while sentence_count < len(input_text_series)-1:\n",
    "        count = 0\n",
    "        text = input_text_series[sentence_count]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            swap_pos = randint(word_count, tokens_length-1)\n",
    "            word_to_swap = input_tokens[swap_pos]\n",
    "            if len(word_to_swap) > 3:\n",
    "                word_to_swap_lemma = WordNetLemmatizer().lemmatize(word_to_swap)\n",
    "                if word_to_swap_lemma not in punctuation:\n",
    "                    if word_to_swap_lemma not in stop_words and not word_to_swap_lemma.isdigit():\n",
    "                        synonym_net = wordnet.synsets(word_to_swap_lemma)\n",
    "                        if len(synonym_net) > 1:\n",
    "                            swap_word = synonym_net[0].lemmas()[0].name()\n",
    "                            if transformer_similarity_score(word_to_swap, swap_word, model) > similarity_tsd:\n",
    "                                if word_to_swap  != swap_word and word_to_swap_lemma  != swap_word:\n",
    "                                    input_tokens[swap_pos] = swap_word\n",
    "                                    mod_text = \" \".join (input_tokens)\n",
    "                                    write_to_csv([text, mod_text, swap_pos, word_to_swap, swap_word],log_file_path)\n",
    "                                    count = count + 1\n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        if cosine_similarity_score(text, mod_text, vectorizer)>similarity_tsd:\n",
    "            input_test_df.at[sentence_count, 'text'] = mod_text\n",
    "            sentence_count = sentence_count + 1\n",
    "    input_test_df.to_csv(output_file_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(input_file_path, word_count, word_length, operation_type):\n",
    "    pmr = 'add_word'\n",
    "    mr = f'{pmr}_{word_count}_{word_length}_{operation_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'added_word'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        count = 0\n",
    "        words_added = []\n",
    "        add_positions = []\n",
    "        text = input_text_series[input_text_index]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            add_pos = randint(count, tokens_length-1)\n",
    "            word_to_add = input_tokens[add_pos]\n",
    "            if word_to_add not in punctuation:\n",
    "                if word_to_add not in stop_words and not word_to_add.isdigit():\n",
    "                    if len(word_to_add) > word_length:\n",
    "                        if operation_type == 'existing':\n",
    "                            input_tokens.insert(add_pos+1, word_to_add)\n",
    "                            words_added.append(word_to_add)\n",
    "                            add_positions.append(add_pos)\n",
    "                            count = count + 1\n",
    "                        elif operation_type == 'new':\n",
    "                            synonym_net = wordnet.synsets(word_to_add)\n",
    "                            if len(synonym_net) > 1:\n",
    "                                add_word = synonym_net[0].lemmas()[0].name()\n",
    "                                if transformer_similarity_score(word_to_add, word_to_add, model) > 0.90:\n",
    "                                    input_tokens.insert(add_pos+1, add_word)\n",
    "                                    words_added.append(add_word)\n",
    "                                    add_positions.append(add_pos)\n",
    "                                    count = count + 1\n",
    "                        else:\n",
    "                            return 'Incorrect operation type'                         \n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, add_positions, words_added],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistake_word(input_file_path, word_count, word_length, character_size, operation_type):\n",
    "    pmr = 'add_word'\n",
    "    mr = f'{pmr}_{word_count}_{word_length}_{character_size}_{operation_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'correct_word', 'misspelled_word'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        count = 0\n",
    "        words_misspelled = []\n",
    "        misspelled_positions = []\n",
    "        correct_words = []\n",
    "        text = input_text_series[input_text_index]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            misspelled_pos = randint(count, tokens_length-1)\n",
    "            word_to_misspell = input_tokens[misspelled_pos]\n",
    "            if len(word_to_misspell) > word_length:\n",
    "                if word_to_misspell not in punctuation:\n",
    "                    if word_to_misspell not in stop_words and not word_to_misspell.isdigit():\n",
    "                        for c in range(character_size):\n",
    "                            list_of_chars = list(word_to_misspell)\n",
    "                            misspelled_char_pos = randint(0, len(list_of_chars)-1)\n",
    "                            if operation_type == 'remove':\n",
    "                                list_of_chars.remove(list_of_chars[misspelled_char_pos])\n",
    "                            if operation_type == 'change':\n",
    "                                randomchar = chr(random.randint(ord('a'), ord('z')))\n",
    "                                list_of_chars.insert(misspelled_char_pos, randomchar)\n",
    "                            if operation_type == 'add':\n",
    "                                list_of_chars.insert(misspelled_char_pos+1, list_of_chars[misspelled_char_pos])\n",
    "                            correct_words.append(word_to_misspell)\n",
    "                            word_to_misspell = \"\".join (list_of_chars)\n",
    "                            input_tokens[misspelled_pos] = word_to_misspell\n",
    "                            words_misspelled.append(word_to_misspell)\n",
    "                            misspelled_positions.append(misspelled_pos)\n",
    "                            count = count + 1\n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, misspelled_positions, correct_words, words_misspelled],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_word(input_file_path, word_count, word_length):\n",
    "    pmr = 'remove_word'\n",
    "    mr = f'{pmr}_{word_count}_{word_length}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'added_word'],log_file_path)\n",
    "    sentence_count = 0\n",
    "    while sentence_count < len(input_text_series)-1:\n",
    "        count = 0\n",
    "        words_removed = []\n",
    "        remove_positions = []\n",
    "        text = input_text_series[sentence_count]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            tokens_length = len(input_tokens)\n",
    "            remove_pos = randint(count, tokens_length-1)\n",
    "            word_to_remove = input_tokens[remove_pos]\n",
    "            if word_to_remove not in punctuation:\n",
    "                if word_to_remove not in stop_words and not word_to_remove.isdigit():\n",
    "                    if len(word_to_remove) > word_length:\n",
    "                        input_tokens.remove(word_to_remove)\n",
    "                        words_removed.append(word_to_remove)\n",
    "                        remove_positions.append(remove_pos) \n",
    "                        count = count + 1                        \n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        if transformer_similarity_score(text, mod_text, model) > 0.90:\n",
    "            input_test_df.at[sentence_count, 'text'] = mod_text\n",
    "            write_to_csv([text, mod_text, remove_positions, words_removed],log_file_path)\n",
    "            sentence_count = sentence_count + 1\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_change(input_file_path, demographic_type, operation_type):\n",
    "    pmr = 'demographic_change'\n",
    "    mr = f'{pmr}_{demographic_type}_{operation_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        if operation_type == 'swap':\n",
    "            if demographic_type == 'gender':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text) or re.search(r'\\sex m\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex m\\b', 'sex f', text)\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'she', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'her', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'herself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'hers', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text) or re.search(r'\\sex f\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex f\\b', 'sex m', text)\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'he', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'him', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'himself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'his', mod_text)\n",
    "                    modified = 'Yes'\n",
    "            if demographic_type == 'pronoun':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'she', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'her', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'herself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'hers', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'he', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'him', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'himself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'his', mod_text)\n",
    "                    modified = 'Yes'\n",
    "        if operation_type == 'replace':\n",
    "            if demographic_type == 'gender':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text) or re.search(r'\\sex m\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex m\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'patient', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text) or re.search(r'\\sex f\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex f\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'patient', mod_text)\n",
    "                    modified = 'Yes'\n",
    "            if demographic_type == 'pronoun':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'patient', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'patient', mod_text)\n",
    "                    modified = 'Yes'\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_demographic(input_file_path, demographic_type):\n",
    "    pmr = 'remove_demographic'\n",
    "    mr = f'{pmr}_{demographic_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        if demographic_type == 'gender':\n",
    "            if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text) or re.search(r'\\sex m\\b', text):\n",
    "                mod_text = re.sub(r'\\bsex m\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bhe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bhim\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhimself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhis\\b', '', mod_text)\n",
    "                text = mod_text\n",
    "                modified = 'Yes'\n",
    "            if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text) or re.search(r'\\sex f\\b', text):\n",
    "                mod_text = re.sub(r'\\bsex f\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bshe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bher\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bherself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhers\\b', '', mod_text)\n",
    "                modified = 'Yes'\n",
    "        if demographic_type == 'pronoun':\n",
    "            if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text):\n",
    "                mod_text = re.sub(r'\\bhe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bhim\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhimself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhis\\b', '', mod_text)\n",
    "                text = mod_text\n",
    "                modified = 'Yes'\n",
    "            if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text):\n",
    "                mod_text = re.sub(r'\\bshe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bher\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bherself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhers\\b', '', mod_text)\n",
    "                modified = 'Yes'\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronym(input_file_path, acronym_dict):\n",
    "    pmr = 'replace_acronym'\n",
    "    mr = f'{pmr}_{len(acronym_dict.keys())}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified', 'num_of_acronyms_mod'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        num_of_acronyms_mod = 0\n",
    "        for acronym in acronym_dict.keys():\n",
    "            mod_text = re.sub(r'\\b{}\\b'.format(acronym), acronym_dict[acronym], mod_text)\n",
    "            modified = 'Yes'\n",
    "            num_of_acronyms_mod += 1\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified, num_of_acronyms_mod],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chemical(input_file_path, chemical_dict):\n",
    "    pmr = 'replace_chemical'\n",
    "    mr = f'{pmr}_{len(chemical_dict.keys())}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified', 'num_of_acronyms_mod'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        num_of_acronyms_mod = 0\n",
    "        for acronym in chemical_dict.keys():\n",
    "            mod_text = re.sub(r'\\b{}\\b'.format(acronym), chemical_dict[acronym], mod_text)\n",
    "            modified = 'Yes'\n",
    "            num_of_acronyms_mod += 1\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified, num_of_acronyms_mod],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score_add_distribution(input_file_path, output_file_path, num_of_words, word_len, operation_type):\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    log_file_name = 'add_change_log-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    log_file_path = '../log/'+log_file_name+'.csv'\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    add_size = num_of_words\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'added_word'],log_file_path)\n",
    "    output_dict = {}\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        count = 0\n",
    "        words_added = []\n",
    "        add_positions = []\n",
    "        text = input_text_series[input_text_index]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < add_size:\n",
    "            add_pos = randint(count, tokens_length-1)\n",
    "            word_to_add = input_tokens[add_pos]\n",
    "            if word_to_add not in punctuation:\n",
    "                if word_to_add not in stop_words and not word_to_add.isdigit():\n",
    "                    if len(word_to_add) > word_len:\n",
    "                        if operation_type == 'existing':\n",
    "                            input_tokens.insert(add_pos+1, word_to_add)\n",
    "                            words_added.append(word_to_add)\n",
    "                            add_positions.append(add_pos)\n",
    "                            count = count + 1\n",
    "                        elif operation_type == 'new':\n",
    "                            synonym_net = wordnet.synsets(word_to_add)\n",
    "                            if len(synonym_net) > 1:\n",
    "                                add_word = synonym_net[0].lemmas()[0].name()\n",
    "                                input_tokens.insert(add_pos+1, add_word)\n",
    "                                words_added.append(add_word)\n",
    "                                add_positions.append(add_pos)\n",
    "                                count = count + 1\n",
    "                        else:\n",
    "                            return 'Incorrect operation type'                         \n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        similarity_Score = transformer_similarity_score(text, mod_text, model)\n",
    "        output_dict[input_text_index] = similarity_Score\n",
    "        write_to_csv([text, mod_text, add_positions, words_added],log_file_path)\n",
    "    output_df = pd.DataFrame.from_dict(output_dict, orient='index', columns=['similarity_Score'])\n",
    "    output_df.to_csv(output_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score_swap_distribution(input_file_path, output_file_path, num_of_words_to_swap):\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    log_file_name = 'swap_change_log-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    log_file_path = '../log/'+log_file_name+'.csv'\n",
    "    word_output_file_path = '../sim_analysis/test_full_swap_mr_word_sim_r3.csv'\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    swap_size = num_of_words_to_swap\n",
    "    write_to_csv(['actual_text', 'modified_text', 'swapped_word_pos', 'word_swapped', 'swapped_word'],log_file_path)\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    sentence_count = 0\n",
    "    word_index = 0\n",
    "    output_dict = {}\n",
    "    output_word_dict = {}\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        word_count = 0\n",
    "        text = input_text_series[sentence_count]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while word_count < swap_size:\n",
    "            swap_pos = randint(word_count, tokens_length-1)\n",
    "            word_to_swap = input_tokens[swap_pos]\n",
    "            if len(word_to_swap) > 3:\n",
    "                word_to_swap_lemma = WordNetLemmatizer().lemmatize(word_to_swap)\n",
    "                if word_to_swap_lemma not in punctuation:\n",
    "                    if word_to_swap_lemma not in stop_words and not word_to_swap_lemma.isdigit():\n",
    "                        synonym_net = wordnet.synsets(word_to_swap_lemma)\n",
    "                        if len(synonym_net) > 1:\n",
    "                            swap_word = synonym_net[0].lemmas()[0].name()\n",
    "                            # if transformer_similarity_score(word_to_swap, swap_word, model) > similarity_tsd:\n",
    "                            \n",
    "                            if word_to_swap  != swap_word and word_to_swap_lemma  != swap_word:\n",
    "                                word_similarity_score = transformer_similarity_score(word_to_swap, swap_word, model)\n",
    "                                output_word_dict[word_index] = word_similarity_score\n",
    "                                word_index = word_index + 1\n",
    "                                output_word_df = pd.DataFrame.from_dict(output_word_dict, orient='index', columns=['similarity_Score'])\n",
    "                                output_word_df.to_csv(word_output_file_path) \n",
    "                                input_tokens[swap_pos] = swap_word\n",
    "                                mod_text = \" \".join (input_tokens)\n",
    "                                write_to_csv([text, mod_text, swap_pos, word_to_swap, swap_word],log_file_path)\n",
    "                                word_count = word_count + 1\n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        sentence_count = sentence_count + 1\n",
    "        similarity_Score = cosine_similarity_score(text, mod_text, TfidfVectorizer())\n",
    "        output_dict[input_text_index] = similarity_Score\n",
    "    input_test_df.to_csv(output_file_path)\n",
    "    output_df = pd.DataFrame.from_dict(output_dict, orient='index', columns=['similarity_Score'])\n",
    "    output_df.to_csv(output_file_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plm-icd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
