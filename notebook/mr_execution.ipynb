{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from nltk) (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence_transformers in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (0.25.0)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pubchempy\n",
      "  Using cached PubChemPy-1.0.4.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pubchempy\n",
      "  Building wheel for pubchempy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pubchempy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13818 sha256=cb472261dc6674276f2e2465fb4ceccfab73664f557f9360321388a9b8075398\n",
      "  Stored in directory: /Users/guna/Library/Caches/pip/wheels/b0/8c/ba/3b00b89931153bf5a4eaa8e73bd1b0319a879cc45175326854\n",
      "Successfully built pubchempy\n",
      "Installing collected packages: pubchempy\n",
      "Successfully installed pubchempy-1.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting chemdataextractor\n",
      "  Using cached ChemDataExtractor-1.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting appdirs (from chemdataextractor)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting beautifulsoup4 (from chemdataextractor)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from chemdataextractor) (8.1.7)\n",
      "Collecting cssselect (from chemdataextractor)\n",
      "  Using cached cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting lxml (from chemdataextractor)\n",
      "  Downloading lxml-5.3.0.tar.gz (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from chemdataextractor) (3.9.1)\n",
      "Collecting pdfminer.six (from chemdataextractor)\n",
      "  Using cached pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from chemdataextractor) (2.9.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from chemdataextractor) (2.32.3)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from chemdataextractor) (1.16.0)\n",
      "Collecting python-crfsuite (from chemdataextractor)\n",
      "  Using cached python-crfsuite-0.9.10.tar.gz (478 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting DAWG (from chemdataextractor)\n",
      "  Using cached DAWG-0.8.0.tar.gz (371 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from chemdataextractor) (6.0.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->chemdataextractor)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from nltk->chemdataextractor) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from nltk->chemdataextractor) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from nltk->chemdataextractor) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from pdfminer.six->chemdataextractor) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->chemdataextractor)\n",
      "  Downloading cryptography-43.0.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from requests->chemdataextractor) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from requests->chemdataextractor) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages (from requests->chemdataextractor) (2024.8.30)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six->chemdataextractor)\n",
      "  Downloading cffi-1.17.1.tar.gz (516 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->chemdataextractor)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Using cached ChemDataExtractor-1.3.0-py3-none-any.whl (182 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
      "Downloading cryptography-43.0.1-cp37-abi3-macosx_10_9_universal2.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Building wheels for collected packages: DAWG, lxml, python-crfsuite, cffi\n",
      "  Building wheel for DAWG (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for DAWG: filename=DAWG-0.8.0-cp38-cp38-macosx_11_0_arm64.whl size=150374 sha256=03e0dcf385f90a0ce5e544331f7e23566c26535290e7fa9f2b54ad15c0a97fb6\n",
      "  Stored in directory: /Users/guna/Library/Caches/pip/wheels/1c/e6/8f/313a7ccc57b29a7affb7205664277a1d5ebe73bf600a69a615\n",
      "  Building wheel for lxml (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lxml: filename=lxml-5.3.0-cp38-cp38-macosx_11_0_arm64.whl size=1484260 sha256=c2cdaedb98f5d509a4e10039909aad1bd29dd0fee84153bed266d5db30988e84\n",
      "  Stored in directory: /Users/guna/Library/Caches/pip/wheels/37/19/50/80ff9642d74365e416a0b370a5784afd3167f4e1182644c30b\n",
      "  Building wheel for python-crfsuite (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-crfsuite: filename=python_crfsuite-0.9.10-cp38-cp38-macosx_11_0_arm64.whl size=173727 sha256=7bbf2dd0a088b89db3a1c1930e1b4a6cfd526b7ce66c1dd5537a39e25f35efdd\n",
      "  Stored in directory: /Users/guna/Library/Caches/pip/wheels/b0/4a/c7/0a42e08efba3e120e173d615f9bba51baba1e07fc380f6beb9\n",
      "  Building wheel for cffi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cffi: filename=cffi-1.17.1-cp38-cp38-macosx_11_0_arm64.whl size=178536 sha256=180a48c9741cfbb68362b6e4fe8e0c76862da88b8b757cfc0ae0602430404449\n",
      "  Stored in directory: /Users/guna/Library/Caches/pip/wheels/f8/73/f0/643f1772516a6bd314c8e57831b7d5204a45e9fe1b876b6b35\n",
      "Successfully built DAWG lxml python-crfsuite cffi\n",
      "Installing collected packages: python-crfsuite, DAWG, appdirs, soupsieve, pycparser, lxml, cssselect, cffi, beautifulsoup4, cryptography, pdfminer.six, chemdataextractor\n",
      "Successfully installed DAWG-0.8.0 appdirs-1.4.4 beautifulsoup4-4.12.3 cffi-1.17.1 chemdataextractor-1.3.0 cryptography-43.0.1 cssselect-1.2.0 lxml-5.3.0 pdfminer.six-20240706 pycparser-2.22 python-crfsuite-0.9.10 soupsieve-2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nltk.download('wordnet')` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install sentence_transformers\n",
    "%pip install pubchempy\n",
    "%pip install chemdataextractor\n",
    "import nltk\n",
    "%nltk.download('wordnet')\n",
    "%nltk.download('omw-1.4')\n",
    "%nltk.download('stopwords')\n",
    "%nltk.download('punkt')\n",
    "%cde data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/guna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from random import randint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# from chemdataextractor import Document\n",
    "# import pubchempy as pubc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(values : list, filepath : str) -> None:\n",
    "    '''Function to write output on the csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values : list\n",
    "      List of values to be written on the csv\n",
    "    filepath : str\n",
    "      Path of the csv file\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    write_to_csv(['value1', 'value2', 'value3'], 'output.csv')\n",
    "    This will append the row ['value1', 'value2', 'value3'] to 'output.csv'.\n",
    "    '''\n",
    "    if not isinstance(values, list) or not values:\n",
    "        raise ValueError(\"The 'values' parameter must be a non-empty list.\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'a', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(values)\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"An error occurred while writing to the file {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text : str) -> str:\n",
    "   '''\n",
    "    Clean up the input data by applying preprocessing steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input string that will be processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The preprocessed string after applying the cleanup operations.\n",
    "    '''\n",
    "   tokens = word_tokenize(text)\n",
    "   tokens = [w for w in tokens if w not in punctuation and not w.isdigit() and not len(w) < 3]\n",
    "   stop_words = stopwords.words ('english')\n",
    "   tweet_without_stopwords = [t for t in tokens if t not in stop_words]\n",
    "   text = \" \".join (tweet_without_stopwords)\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_score(s1,s2, vec):\n",
    "    '''\n",
    "    Calculate the cosine similarity score between two input strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input string on which preprocessing should be applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The preprocessed string. \n",
    "    '''\n",
    "    s1 = preprocessing(s1)\n",
    "    s2 = preprocessing(s2)\n",
    "    sentences = [s1,s2]\n",
    "    sentence_to_vec = vec.fit_transform(sentences)\n",
    "    sentence_to_vec_arr = sentence_to_vec.toarray()\n",
    "    sim_score = cosine_similarity(sentence_to_vec_arr)\n",
    "    return round(sim_score[0][1],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_similarity_score(s1,s2, model) -> float:\n",
    "    '''\n",
    "    Computes the semantic similarity score between two input words/strings using a transformer model.\n",
    "\n",
    "    Args:\n",
    "        s1 (str): The first input word/string.\n",
    "        s2 (str): The second input word/string.\n",
    "        model (obj): The transformer model used to encode the strings into embeddings. \n",
    "            This model should have an `encode` method that returns tensor embeddings.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity score between the embeddings of the two input word/strings,\n",
    "            rounded to three decimal places. The score ranges from 0.0 (completely dissimilar)\n",
    "            to 1.0 (completely similar).\n",
    "    '''\n",
    "    embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "    semantic_similarity_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    return round(float(semantic_similarity_scores[0][0]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe(input_file_path : str) -> pd.DataFrame: \n",
    "  if 'json' in input_file_path:\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        json_input = json.load(file)\n",
    "  else:\n",
    "    raise IOError(f\"{input_file_path.split('.')[-1]} is an invalid input file format\")\n",
    "  df_out = pd.DataFrame.from_dict(json_input, orient='columns')\n",
    "  df_out = df_out.reset_index(drop=True)\n",
    "  return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_dataframe(input_file_path : str) -> pd.DataFrame:\n",
    "  if 'jsonl' in input_file_path:\n",
    "         jsonl_input = [json.loads(l) for l in open(input_file_path, \"r\")]\n",
    "  else:\n",
    "    raise IOError(f\"{input_file_path.split('.')[-1]} is an invalid input file format\")\n",
    "  df_out = pd.DataFrame.from_dict(jsonl_input, orient='columns')\n",
    "  df_out = df_out.reset_index(drop=True)\n",
    "  return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_dataframe_to_json(input_df : pd.DataFrame) -> dict:\n",
    "  if isinstance(input_df, pd.DataFrame):\n",
    "    return pd.DataFrame.to_dict(input_df, orient='records')\n",
    "  else:\n",
    "    TypeError(\"Provided input is not a pandas DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframe_to_jsonl(input_df : pd.DataFrame, output_file_path : str) -> None:\n",
    "  if isinstance(input_df, pd.DataFrame):\n",
    "    input_dict = pd.DataFrame.to_dict(input_df, orient='records')\n",
    "    for record in input_dict:\n",
    "      with open(output_file_path, 'a') as outfile:\n",
    "          json_line = json.dumps(record)\n",
    "          outfile.write(json_line + '\\n')\n",
    "  else:\n",
    "    TypeError(\"Provided input is not a pandas DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(input_file_path, word_count, similarity_tsd):\n",
    "    pmr = 'swap_word'\n",
    "    mr = f'{pmr}_{word_count}_{similarity_tsd}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'swapped_word_pos', 'word_swapped', 'swapped_word'],log_file_path)\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    sentence_count = 0\n",
    "    while sentence_count < len(input_text_series)-1:\n",
    "        count = 0\n",
    "        text = input_text_series[sentence_count]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            swap_pos = randint(word_count, tokens_length-1)\n",
    "            word_to_swap = input_tokens[swap_pos]\n",
    "            if len(word_to_swap) > 3:\n",
    "                word_to_swap_lemma = WordNetLemmatizer().lemmatize(word_to_swap)\n",
    "                if word_to_swap_lemma not in punctuation:\n",
    "                    if word_to_swap_lemma not in stop_words and not word_to_swap_lemma.isdigit():\n",
    "                        synonym_net = wordnet.synsets(word_to_swap_lemma)\n",
    "                        if len(synonym_net) > 1:\n",
    "                            swap_word = synonym_net[0].lemmas()[0].name()\n",
    "                            if transformer_similarity_score(word_to_swap, swap_word, model) > similarity_tsd:\n",
    "                                if word_to_swap  != swap_word and word_to_swap_lemma  != swap_word:\n",
    "                                    input_tokens[swap_pos] = swap_word\n",
    "                                    mod_text = \" \".join (input_tokens)\n",
    "                                    write_to_csv([text, mod_text, swap_pos, word_to_swap, swap_word],log_file_path)\n",
    "                                    count = count + 1\n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        if cosine_similarity_score(text, mod_text, vectorizer)>similarity_tsd:\n",
    "            input_test_df.at[sentence_count, 'text'] = mod_text\n",
    "            sentence_count = sentence_count + 1\n",
    "    input_test_df.to_csv(output_file_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(original_text, word_count, similarity_tsd):\n",
    "    pmr = 'swap_word'\n",
    "    mr = f'{pmr}_{word_count}_{similarity_tsd}'\n",
    "    # mod_input_dir = f'../modified_input/{pmr}'\n",
    "    # log_dir = f'../log/{pmr}'\n",
    "    # # if not os.path.exists(mod_input_dir):\n",
    "    # #     os.makedirs(mod_input_dir)\n",
    "    # # output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    # log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    # if not os.path.exists(log_dir):\n",
    "    #     os.makedirs(log_dir)\n",
    "    # log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    # input_test_df = pd.read_csv(input_file_path)\n",
    "    # input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    # write_to_csv(['actual_text', 'modified_text', 'swapped_word_pos', 'word_swapped', 'swapped_word'],log_file_path)\n",
    "    sentence_count = 0\n",
    "    max_iter = 0\n",
    "    while sentence_count < 1:\n",
    "        modification_status = 'no_mod'\n",
    "        count = 0\n",
    "        text = original_text\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            if max_iter < 300:\n",
    "                swap_pos = randint(word_count, tokens_length-1)\n",
    "                word_to_swap = input_tokens[swap_pos]\n",
    "                if len(word_to_swap) > 3:\n",
    "                    word_to_swap_lemma = WordNetLemmatizer().lemmatize(word_to_swap)\n",
    "                    if word_to_swap_lemma not in punctuation:\n",
    "                        if word_to_swap_lemma not in stop_words and not word_to_swap_lemma.isdigit():\n",
    "                            synonym_net = wordnet.synsets(word_to_swap_lemma)\n",
    "                            # print(synonym_net)\n",
    "                            if len(synonym_net) > 1:\n",
    "                                swap_word = synonym_net[0].lemmas()[0].name()\n",
    "                                # print(word_to_swap, swap_word)\n",
    "                                if transformer_similarity_score(word_to_swap, swap_word, model) > similarity_tsd:\n",
    "                                    max_iter += 1\n",
    "                                    if word_to_swap  != swap_word and word_to_swap_lemma  != swap_word:\n",
    "                                        input_tokens[swap_pos] = swap_word\n",
    "                                        mod_text = \" \".join (input_tokens)\n",
    "                                        # print(word_to_swap, swap_word)\n",
    "                                        #   write_to_csv([text, mod_text, swap_pos, word_to_swap, swap_word],log_file_path)\n",
    "                                        count = count + 1\n",
    "            elif count > 1:\n",
    "                modification_status = 'partial'\n",
    "                return mod_text, modification_status\n",
    "            else: return text, modification_status\n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        if cosine_similarity_score(text, mod_text, vectorizer)>similarity_tsd:\n",
    "                # input_test_df.at[sentence_count, 'text'] = mod_text\n",
    "            sentence_count = sentence_count + 1\n",
    "            modification_status = 'complete'\n",
    "            return mod_text, modification_status\n",
    "    # input_test_df.to_csv(output_file_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(input_dataframe, column_name,word_count, word_length, operation_type):\n",
    "    pmr = 'add_word'\n",
    "    mr = f'{pmr}_{word_count}_{word_length}_{operation_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = input_dataframe\n",
    "    input_text_series = input_test_df[column_name]\n",
    "    stop_words = stopwords.words ('english')\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'added_word'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        count = 0\n",
    "        words_added = []\n",
    "        add_positions = []\n",
    "        text = input_text_series[input_text_index]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            add_pos = randint(count, tokens_length-1)\n",
    "            word_to_add = input_tokens[add_pos]\n",
    "            if word_to_add not in punctuation:\n",
    "                if word_to_add not in stop_words and not word_to_add.isdigit():\n",
    "                    if len(word_to_add) > word_length:\n",
    "                        if operation_type == 'existing':\n",
    "                            input_tokens.insert(add_pos+1, word_to_add)\n",
    "                            words_added.append(word_to_add)\n",
    "                            add_positions.append(add_pos)\n",
    "                            count = count + 1\n",
    "                        elif operation_type == 'new':\n",
    "                            synonym_net = wordnet.synsets(word_to_add)\n",
    "                            if len(synonym_net) > 1:\n",
    "                                add_word = synonym_net[0].lemmas()[0].name()\n",
    "                                if transformer_similarity_score(word_to_add, word_to_add, model) > 0.90:\n",
    "                                    input_tokens.insert(add_pos+1, add_word)\n",
    "                                    words_added.append(add_word)\n",
    "                                    add_positions.append(add_pos)\n",
    "                                    count = count + 1\n",
    "                        else:\n",
    "                            return 'Incorrect operation type'                         \n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        input_test_df.at[input_text_index, column_name] = mod_text\n",
    "        write_to_csv([text, mod_text, add_positions, words_added],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/evalllm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "original_file_path = '../original_input/MedInstruct-test.jsonl'\n",
    "df = jsonl_to_dataframe(original_file_path)\n",
    "add_word(df, 'instruction' ,3, 3, 'existing')\n",
    "# write_dataframe_to_jsonl(df, '../modified_input/MedInstruct-test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_file_path = '../modified_input/add_word'\n",
    "df = pd.read_csv(modified_file_path+'/add_word_3_3_existing.csv', index_col=False)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "write_dataframe_to_jsonl(df, modified_file_path+'/MedInstruct_add_word_3_3_existing.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistake_word(input_dataframe, column_name, word_count, word_length, character_size, operation_type):\n",
    "    pmr = 'mistake_word'\n",
    "    mr = f'{pmr}_{word_count}_{word_length}_{character_size}_{operation_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = input_dataframe\n",
    "    input_text_series = input_test_df[column_name]\n",
    "    stop_words = stopwords.words ('english')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'correct_word', 'misspelled_word'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        count = 0\n",
    "        words_misspelled = []\n",
    "        misspelled_positions = []\n",
    "        correct_words = []\n",
    "        text = input_text_series[input_text_index]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            misspelled_pos = randint(count, tokens_length-1)\n",
    "            word_to_misspell = input_tokens[misspelled_pos]\n",
    "            if len(word_to_misspell) > word_length:\n",
    "                if word_to_misspell not in punctuation:\n",
    "                    if word_to_misspell not in stop_words and not word_to_misspell.isdigit():\n",
    "                        for c in range(character_size):\n",
    "                            list_of_chars = list(word_to_misspell)\n",
    "                            misspelled_char_pos = randint(0, len(list_of_chars)-1)\n",
    "                            if operation_type == 'remove':\n",
    "                                list_of_chars.remove(list_of_chars[misspelled_char_pos])\n",
    "                            if operation_type == 'change':\n",
    "                                randomchar = chr(random.randint(ord('a'), ord('z')))\n",
    "                                list_of_chars.insert(misspelled_char_pos, randomchar)\n",
    "                            if operation_type == 'add':\n",
    "                                list_of_chars.insert(misspelled_char_pos+1, list_of_chars[misspelled_char_pos])\n",
    "                        correct_words.append(word_to_misspell)\n",
    "                        word_to_misspell = \"\".join (list_of_chars)\n",
    "                        input_tokens[misspelled_pos] = word_to_misspell\n",
    "                        words_misspelled.append(word_to_misspell)\n",
    "                        misspelled_positions.append(misspelled_pos)\n",
    "                        count = count + 1\n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        input_test_df.at[input_text_index, column_name] = mod_text\n",
    "        write_to_csv([text, mod_text, misspelled_positions, correct_words, words_misspelled],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_path = '../original_input/MedInstruct-test.jsonl'\n",
    "df = jsonl_to_dataframe(original_file_path)\n",
    "mistake_word(df, 'instruction' ,2, 3, 2, 'change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_file_path = '../modified_input/mistake_word'\n",
    "df = pd.read_csv(modified_file_path+'/mistake_word_2_3_2_change.csv', index_col=False)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "write_dataframe_to_jsonl(df, modified_file_path+'/MedInstruct_mistake_word_2_3_2_change.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_word(input_file_path, word_count, word_length):\n",
    "    pmr = 'remove_word'\n",
    "    mr = f'{pmr}_{word_count}_{word_length}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'added_word'],log_file_path)\n",
    "    sentence_count = 0\n",
    "    while sentence_count < len(input_text_series)-1:\n",
    "        count = 0\n",
    "        words_removed = []\n",
    "        remove_positions = []\n",
    "        text = input_text_series[sentence_count]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < word_count:\n",
    "            tokens_length = len(input_tokens)\n",
    "            remove_pos = randint(count, tokens_length-1)\n",
    "            word_to_remove = input_tokens[remove_pos]\n",
    "            if word_to_remove not in punctuation:\n",
    "                if word_to_remove not in stop_words and not word_to_remove.isdigit():\n",
    "                    if len(word_to_remove) > word_length:\n",
    "                        input_tokens.remove(word_to_remove)\n",
    "                        words_removed.append(word_to_remove)\n",
    "                        remove_positions.append(remove_pos) \n",
    "                        count = count + 1                        \n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        if transformer_similarity_score(text, mod_text, model) > 0.90:\n",
    "            input_test_df.at[sentence_count, 'text'] = mod_text\n",
    "            write_to_csv([text, mod_text, remove_positions, words_removed],log_file_path)\n",
    "            sentence_count = sentence_count + 1\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_change(input_file_path, demographic_type, operation_type):\n",
    "    pmr = 'demographic_change'\n",
    "    mr = f'{pmr}_{demographic_type}_{operation_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        if operation_type == 'swap':\n",
    "            if demographic_type == 'gender':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text) or re.search(r'\\sex m\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex m\\b', 'sex f', text)\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'she', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'her', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'herself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'hers', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text) or re.search(r'\\sex f\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex f\\b', 'sex m', text)\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'he', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'him', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'himself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'his', mod_text)\n",
    "                    modified = 'Yes'\n",
    "            if demographic_type == 'pronoun':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'she', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'her', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'herself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'hers', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'he', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'him', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'himself', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'his', mod_text)\n",
    "                    modified = 'Yes'\n",
    "        if operation_type == 'replace':\n",
    "            if demographic_type == 'gender':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text) or re.search(r'\\sex m\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex m\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'patient', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text) or re.search(r'\\sex f\\b', text):\n",
    "                    mod_text = re.sub(r'\\bsex f\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'patient', mod_text)\n",
    "                    modified = 'Yes'\n",
    "            if demographic_type == 'pronoun':\n",
    "                if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bhe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bhim\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhimself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhis\\b', 'patient', mod_text)\n",
    "                    text = mod_text\n",
    "                    modified = 'Yes'\n",
    "                if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text):\n",
    "                    mod_text = re.sub(r'\\bshe\\b', 'patient', text)\n",
    "                    mod_text = re.sub(r'\\bher\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bherself\\b', 'patient', mod_text)\n",
    "                    mod_text = re.sub(r'\\bhers\\b', 'patient', mod_text)\n",
    "                    modified = 'Yes'\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_demographic(input_file_path, demographic_type):\n",
    "    pmr = 'remove_demographic'\n",
    "    mr = f'{pmr}_{demographic_type}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        if demographic_type == 'gender':\n",
    "            if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text) or re.search(r'\\sex m\\b', text):\n",
    "                mod_text = re.sub(r'\\bsex m\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bhe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bhim\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhimself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhis\\b', '', mod_text)\n",
    "                text = mod_text\n",
    "                modified = 'Yes'\n",
    "            if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text) or re.search(r'\\sex f\\b', text):\n",
    "                mod_text = re.sub(r'\\bsex f\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bshe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bher\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bherself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhers\\b', '', mod_text)\n",
    "                modified = 'Yes'\n",
    "        if demographic_type == 'pronoun':\n",
    "            if re.search(r'\\bhe\\b', text) or re.search(r'\\bhim\\b', text) or re.search(r'\\bhis\\b', text) or re.search(r'\\bhimself\\b', text):\n",
    "                mod_text = re.sub(r'\\bhe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bhim\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhimself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhis\\b', '', mod_text)\n",
    "                text = mod_text\n",
    "                modified = 'Yes'\n",
    "            if re.search(r'\\bshe\\b', text) or re.search(r'\\bher\\b', text) or re.search(r'\\bhers\\b', text) or re.search(r'\\bherself\\b', text):\n",
    "                mod_text = re.sub(r'\\bshe\\b', '', text)\n",
    "                mod_text = re.sub(r'\\bher\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bherself\\b', '', mod_text)\n",
    "                mod_text = re.sub(r'\\bhers\\b', '', mod_text)\n",
    "                modified = 'Yes'\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronym(input_file_path, acronym_dict):\n",
    "    pmr = 'replace_acronym'\n",
    "    mr = f'{pmr}_{len(acronym_dict.keys())}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified', 'num_of_acronyms_mod'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        num_of_acronyms_mod = 0\n",
    "        for acronym in acronym_dict.keys():\n",
    "            mod_text = re.sub(r'\\b{}\\b'.format(acronym), acronym_dict[acronym], mod_text)\n",
    "            modified = 'Yes'\n",
    "            num_of_acronyms_mod += 1\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified, num_of_acronyms_mod],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chemical(input_file_path, chemical_dict):\n",
    "    pmr = 'replace_chemical'\n",
    "    mr = f'{pmr}_{len(chemical_dict.keys())}'\n",
    "    mod_input_dir = f'../modified_input/{pmr}'\n",
    "    log_dir = f'../log/{pmr}'\n",
    "    if not os.path.exists(mod_input_dir):\n",
    "        os.makedirs(mod_input_dir)\n",
    "    output_file_path = f'{mod_input_dir}/{mr}.csv'\n",
    "    log_file_name = mr+'-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file_path = f'{log_dir}/{log_file_name}.csv'\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    input_text_series = input_test_df['text']\n",
    "    write_to_csv(['actual_text', 'modified_text', 'is_mofified', 'num_of_acronyms_mod'],log_file_path)\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        text = input_text_series[input_text_index]\n",
    "        mod_text = text\n",
    "        modified = 'No'\n",
    "        num_of_acronyms_mod = 0\n",
    "        for acronym in chemical_dict.keys():\n",
    "            mod_text = re.sub(r'\\b{}\\b'.format(acronym), chemical_dict[acronym], mod_text)\n",
    "            modified = 'Yes'\n",
    "            num_of_acronyms_mod += 1\n",
    "        input_test_df.at[input_text_index, 'text'] = mod_text\n",
    "        write_to_csv([text, mod_text, modified, num_of_acronyms_mod],log_file_path)\n",
    "    input_test_df.to_csv(output_file_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score_add_distribution(input_file_path, output_file_path, num_of_words, word_len, operation_type):\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    log_file_name = 'add_change_log-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    log_file_path = '../log/'+log_file_name+'.csv'\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    add_size = num_of_words\n",
    "    write_to_csv(['actual_text', 'modified_text', 'word_pos', 'added_word'],log_file_path)\n",
    "    output_dict = {}\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        count = 0\n",
    "        words_added = []\n",
    "        add_positions = []\n",
    "        text = input_text_series[input_text_index]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while count < add_size:\n",
    "            add_pos = randint(count, tokens_length-1)\n",
    "            word_to_add = input_tokens[add_pos]\n",
    "            if word_to_add not in punctuation:\n",
    "                if word_to_add not in stop_words and not word_to_add.isdigit():\n",
    "                    if len(word_to_add) > word_len:\n",
    "                        if operation_type == 'existing':\n",
    "                            input_tokens.insert(add_pos+1, word_to_add)\n",
    "                            words_added.append(word_to_add)\n",
    "                            add_positions.append(add_pos)\n",
    "                            count = count + 1\n",
    "                        elif operation_type == 'new':\n",
    "                            synonym_net = wordnet.synsets(word_to_add)\n",
    "                            if len(synonym_net) > 1:\n",
    "                                add_word = synonym_net[0].lemmas()[0].name()\n",
    "                                input_tokens.insert(add_pos+1, add_word)\n",
    "                                words_added.append(add_word)\n",
    "                                add_positions.append(add_pos)\n",
    "                                count = count + 1\n",
    "                        else:\n",
    "                            return 'Incorrect operation type'                         \n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        similarity_Score = transformer_similarity_score(text, mod_text, model)\n",
    "        output_dict[input_text_index] = similarity_Score\n",
    "        write_to_csv([text, mod_text, add_positions, words_added],log_file_path)\n",
    "    output_df = pd.DataFrame.from_dict(output_dict, orient='index', columns=['similarity_Score'])\n",
    "    output_df.to_csv(output_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score_swap_distribution(input_file_path, output_file_path, num_of_words_to_swap):\n",
    "    input_test_df = pd.read_csv(input_file_path)\n",
    "    log_file_name = 'swap_change_log-{date:%Y-%m-%d_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "    log_file_path = '../log/'+log_file_name+'.csv'\n",
    "    word_output_file_path = '../sim_analysis/test_full_swap_mr_word_sim_r3.csv'\n",
    "    input_text_series = input_test_df['text']\n",
    "    stop_words = stopwords.words ('english')\n",
    "    swap_size = num_of_words_to_swap\n",
    "    write_to_csv(['actual_text', 'modified_text', 'swapped_word_pos', 'word_swapped', 'swapped_word'],log_file_path)\n",
    "    model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "    sentence_count = 0\n",
    "    word_index = 0\n",
    "    output_dict = {}\n",
    "    output_word_dict = {}\n",
    "    for input_text_index in range(len(input_text_series)):\n",
    "        word_count = 0\n",
    "        text = input_text_series[sentence_count]\n",
    "        input_tokens = word_tokenize(text)\n",
    "        tokens_length = len(input_tokens)\n",
    "        while word_count < swap_size:\n",
    "            swap_pos = randint(word_count, tokens_length-1)\n",
    "            word_to_swap = input_tokens[swap_pos]\n",
    "            if len(word_to_swap) > 3:\n",
    "                word_to_swap_lemma = WordNetLemmatizer().lemmatize(word_to_swap)\n",
    "                if word_to_swap_lemma not in punctuation:\n",
    "                    if word_to_swap_lemma not in stop_words and not word_to_swap_lemma.isdigit():\n",
    "                        synonym_net = wordnet.synsets(word_to_swap_lemma)\n",
    "                        if len(synonym_net) > 1:\n",
    "                            swap_word = synonym_net[0].lemmas()[0].name()\n",
    "                            # if transformer_similarity_score(word_to_swap, swap_word, model) > similarity_tsd:\n",
    "                            \n",
    "                            if word_to_swap  != swap_word and word_to_swap_lemma  != swap_word:\n",
    "                                word_similarity_score = transformer_similarity_score(word_to_swap, swap_word, model)\n",
    "                                output_word_dict[word_index] = word_similarity_score\n",
    "                                word_index = word_index + 1\n",
    "                                output_word_df = pd.DataFrame.from_dict(output_word_dict, orient='index', columns=['similarity_Score'])\n",
    "                                output_word_df.to_csv(word_output_file_path) \n",
    "                                input_tokens[swap_pos] = swap_word\n",
    "                                mod_text = \" \".join (input_tokens)\n",
    "                                write_to_csv([text, mod_text, swap_pos, word_to_swap, swap_word],log_file_path)\n",
    "                                word_count = word_count + 1\n",
    "        mod_text = \" \".join (input_tokens)\n",
    "        sentence_count = sentence_count + 1\n",
    "        similarity_Score = cosine_similarity_score(text, mod_text, TfidfVectorizer())\n",
    "        output_dict[input_text_index] = similarity_Score\n",
    "    input_test_df.to_csv(output_file_path)\n",
    "    output_df = pd.DataFrame.from_dict(output_dict, orient='index', columns=['similarity_Score'])\n",
    "    output_df.to_csv(output_file_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpacare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('TimKond/S-PubMedBert-MedQuAD')\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "original_file_path = '../original_input/MedInstruct-test.jsonl'\n",
    "modified_file_path = '../modified_input/MedInstruct-test-swap-3-60.jsonl'\n",
    "mod_status_list = []\n",
    "if 'jsonl' in original_file_path:\n",
    "         original_dataset = [json.loads(l) for l in open(original_file_path, \"r\")]\n",
    "for id in range(len(original_dataset)):\n",
    "    input_dict = original_dataset[id]\n",
    "    original_text = input_dict['instruction']\n",
    "    # print(original_text)\n",
    "    perturbed_text, mod_status = swap_word(original_text, 2, 0.60)\n",
    "    input_dict['instruction'] = perturbed_text\n",
    "    mod_status_list.append(mod_status)\n",
    "    # print(input_dict)\n",
    "    with open(modified_file_path, 'a') as outfile:\n",
    "        json_line = json.dumps(input_dict)\n",
    "        outfile.write(json_line + '\\n')\n",
    "        # json.dump(\\n', outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'complete': 170, 'no_mod': 46})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(mod_status_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the potential side effects for an asthma patient picking prednisone?\n",
    "original_text = \"A 50-year-old male presents chest pain and shortness of breath after physical exertion. With a family history of heart disease, what tests should he probably undergo?\"\n",
    "swap_word(original_text, 2, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/guna/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plm-icd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
